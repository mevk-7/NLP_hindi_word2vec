{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import gensim\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_filenames = sorted(glob.glob(\"final.txt\"))\n",
    "print(\"Found books:\")\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'final.txt'...\n",
      "Corpus is now 14981857 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    corpus = list(sentences.split('.'))\n",
    "    for i in corpus:\n",
    "        i.replace('\\n',' ')\n",
    "    return corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' इस विधा में व्यक्त हुई कविता\\nगेय होती है', ' इसलिए गीत की पहचान उसकी गेयता में निहित होती है', ' इसलिए गीत तभी गीत\\nहै जब वह गेय हो', ' इसलिए जो कविता गेय नहीं है वह गीत नहीं है']\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = tokenize(corpus_raw)\n",
    "print(raw_sentences[1:5])\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.compile(\"[^\\p{Zs}\\p{P}]\")\n",
    "    words = clean.sub(\" \",raw)\n",
    "    #words = re.sub(\"[^\\p{Zs}\\p{P}]\",\" \",raw)\n",
    "    words = words.split()\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        #sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "        sentences.append(raw_sentence.split())\n",
    "#print(raw_sentences[5])\n",
    "#print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-16 19:35:59,488 : INFO : collecting all words and their counts\n",
      "2018-02-16 19:35:59,490 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-16 19:35:59,546 : INFO : PROGRESS: at sentence #10000, processed 168256 words, keeping 23234 word types\n",
      "2018-02-16 19:35:59,596 : INFO : PROGRESS: at sentence #20000, processed 335983 words, keeping 38498 word types\n",
      "2018-02-16 19:35:59,648 : INFO : PROGRESS: at sentence #30000, processed 497512 words, keeping 51255 word types\n",
      "2018-02-16 19:35:59,681 : INFO : PROGRESS: at sentence #40000, processed 649825 words, keeping 61830 word types\n",
      "2018-02-16 19:35:59,738 : INFO : PROGRESS: at sentence #50000, processed 803572 words, keeping 72331 word types\n",
      "2018-02-16 19:35:59,779 : INFO : PROGRESS: at sentence #60000, processed 969289 words, keeping 84228 word types\n",
      "2018-02-16 19:35:59,840 : INFO : PROGRESS: at sentence #70000, processed 1129603 words, keeping 94058 word types\n",
      "2018-02-16 19:35:59,889 : INFO : PROGRESS: at sentence #80000, processed 1291572 words, keeping 103287 word types\n",
      "2018-02-16 19:35:59,935 : INFO : PROGRESS: at sentence #90000, processed 1446616 words, keeping 113510 word types\n",
      "2018-02-16 19:35:59,984 : INFO : PROGRESS: at sentence #100000, processed 1611833 words, keeping 122570 word types\n",
      "2018-02-16 19:36:00,045 : INFO : PROGRESS: at sentence #110000, processed 1773754 words, keeping 133099 word types\n",
      "2018-02-16 19:36:00,094 : INFO : PROGRESS: at sentence #120000, processed 1924913 words, keeping 140828 word types\n",
      "2018-02-16 19:36:00,141 : INFO : PROGRESS: at sentence #130000, processed 2094079 words, keeping 148239 word types\n",
      "2018-02-16 19:36:00,192 : INFO : PROGRESS: at sentence #140000, processed 2240742 words, keeping 155448 word types\n",
      "2018-02-16 19:36:00,248 : INFO : PROGRESS: at sentence #150000, processed 2403525 words, keeping 163267 word types\n",
      "2018-02-16 19:36:00,300 : INFO : PROGRESS: at sentence #160000, processed 2571240 words, keeping 171347 word types\n",
      "2018-02-16 19:36:00,346 : INFO : PROGRESS: at sentence #170000, processed 2732651 words, keeping 179628 word types\n",
      "2018-02-16 19:36:00,396 : INFO : collected 184950 word types from a corpus of 2859115 raw words and 177647 sentences\n",
      "2018-02-16 19:36:00,397 : INFO : Loading a fresh vocabulary\n",
      "2018-02-16 19:36:00,506 : INFO : min_count=5 retains 27700 unique words (14% of original 184950, drops 157250)\n",
      "2018-02-16 19:36:00,507 : INFO : min_count=5 leaves 2639954 word corpus (92% of original 2859115, drops 219161)\n",
      "2018-02-16 19:36:00,602 : INFO : deleting the raw counts dictionary of 184950 items\n",
      "2018-02-16 19:36:00,609 : INFO : sample=0.001 downsamples 34 most-common words\n",
      "2018-02-16 19:36:00,610 : INFO : downsampling leaves estimated 2100672 word corpus (79.6% of prior 2639954)\n",
      "2018-02-16 19:36:00,735 : INFO : estimated required memory for 27700 words and 100 dimensions: 36010000 bytes\n",
      "2018-02-16 19:36:00,736 : INFO : resetting layer weights\n",
      "2018-02-16 19:36:01,054 : INFO : training model with 3 workers on 27700 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-16 19:36:02,064 : INFO : EPOCH 1 - PROGRESS: at 31.64% examples, 667826 words/s, in_qsize 4, out_qsize 1\n",
      "2018-02-16 19:36:03,067 : INFO : EPOCH 1 - PROGRESS: at 56.22% examples, 589218 words/s, in_qsize 4, out_qsize 1\n",
      "2018-02-16 19:36:04,074 : INFO : EPOCH 1 - PROGRESS: at 89.88% examples, 625981 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-16 19:36:04,299 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-16 19:36:04,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-16 19:36:04,302 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-16 19:36:04,303 : INFO : EPOCH - 1 : training on 2859115 raw words (2100233 effective words) took 3.2s, 647963 effective words/s\n",
      "2018-02-16 19:36:05,317 : INFO : EPOCH 2 - PROGRESS: at 43.92% examples, 923320 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-16 19:36:06,323 : INFO : EPOCH 2 - PROGRESS: at 88.35% examples, 920847 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-16 19:36:06,583 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-16 19:36:06,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-16 19:36:06,586 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-16 19:36:06,589 : INFO : EPOCH - 2 : training on 2859115 raw words (2100516 effective words) took 2.3s, 923370 effective words/s\n",
      "2018-02-16 19:36:07,606 : INFO : EPOCH 3 - PROGRESS: at 43.62% examples, 914282 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-16 19:36:08,611 : INFO : EPOCH 3 - PROGRESS: at 87.64% examples, 912642 words/s, in_qsize 4, out_qsize 1\n",
      "2018-02-16 19:36:09,185 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-16 19:36:09,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-16 19:36:09,204 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-16 19:36:09,206 : INFO : EPOCH - 3 : training on 2859115 raw words (2100673 effective words) took 2.6s, 806347 effective words/s\n",
      "2018-02-16 19:36:10,216 : INFO : EPOCH 4 - PROGRESS: at 40.88% examples, 860064 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-16 19:36:11,232 : INFO : EPOCH 4 - PROGRESS: at 76.12% examples, 793458 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-16 19:36:11,884 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-16 19:36:11,885 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-16 19:36:11,886 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-16 19:36:11,887 : INFO : EPOCH - 4 : training on 2859115 raw words (2100637 effective words) took 2.7s, 786116 effective words/s\n",
      "2018-02-16 19:36:12,900 : INFO : EPOCH 5 - PROGRESS: at 38.26% examples, 804880 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-16 19:36:13,903 : INFO : EPOCH 5 - PROGRESS: at 69.73% examples, 726627 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-16 19:36:14,578 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-16 19:36:14,583 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-16 19:36:14,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-16 19:36:14,586 : INFO : EPOCH - 5 : training on 2859115 raw words (2100336 effective words) took 2.7s, 780611 effective words/s\n",
      "2018-02-16 19:36:14,586 : INFO : training on a 14295575 raw words (10502395 effective words) took 13.5s, 776155 effective words/s\n",
      "2018-02-16 19:36:14,587 : INFO : saving Word2Vec object under hindi_model, separately None\n",
      "2018-02-16 19:36:14,588 : INFO : not storing attribute vectors_norm\n",
      "2018-02-16 19:36:14,590 : INFO : not storing attribute cum_table\n",
      "2018-02-16 19:36:14,876 : INFO : saved hindi_model\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=5)\n",
    "model.save(\"hindi_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('hindi_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 27700\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\\nall_word_vectors_matrix = model.syn0\\nall_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize data\n",
    "'''\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\n",
    "all_word_vectors_matrix = model.syn0\n",
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikrant/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2018-02-16 19:36:15,166 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('चरित्र', 0.820863664150238),\n",
       " ('प्राणी', 0.8205936551094055),\n",
       " ('कवि', 0.8179870843887329),\n",
       " ('देवता', 0.8105733394622803),\n",
       " ('राग', 0.80824875831604),\n",
       " ('सौन्दर्य', 0.8081085681915283),\n",
       " ('कलाकार', 0.80555659532547),\n",
       " ('अक्षर', 0.8052080869674683),\n",
       " ('पाठक', 0.8043116331100464),\n",
       " ('वाक्य', 0.802873432636261)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"गीत\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikrant/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('दार्शनिक', 0.8665556907653809),\n",
       " ('लोक', 0.8632512092590332),\n",
       " ('हिन्दू', 0.8481694459915161),\n",
       " ('साहित्यिक', 0.8432353734970093),\n",
       " ('राजनीतिक', 0.8404690027236938),\n",
       " ('सामाजिक', 0.839998185634613),\n",
       " ('सांस्कृतिक', 0.8352468609809875),\n",
       " ('आध्यात्मिक', 0.8276076316833496),\n",
       " ('पाश्चात्य', 0.824255645275116),\n",
       " ('विज्ञान', 0.8210886716842651)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('धार्मिक')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
